{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "python3.7",
      "language": "python",
      "name": "python3.7"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Practice1_LinReg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Smfun12/LinearRegression/blob/main/Practice1_LinReg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5JVX1EZGfIe"
      },
      "source": [
        "## Practice 1: Linear Regression <br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT0uRiv5GfIe"
      },
      "source": [
        "1. Read data, analyze it, split it into train and test set using scikit-learn train_test_split. Read about why we do this.\n",
        "[why split the data](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/) <br>\n",
        "2. Implement your own linear regression. (numpy or torch)<br>\n",
        "3. Try calculating it using <b>Normal equation</b> VS <b>Gradient descent</b>. Try implementing it using only the formulas. Which one is faster? In which situation is gradient descent faster than normal equation?<br>\n",
        "4. Compare performance to scikit-learn LinearRegression. <br>\n",
        "5. Try adding in L1 or L2 regularization. Try different regularization weights. Does it help? Read about regularization and why we do it. [regularization](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) <br>\n",
        "6. Try scaling your data using scikit learn StandardScaler or other techniques [data scaling](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)\n",
        "\n",
        "Might find useful [hands on ML](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BErz7SwGfIe"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khnMOMauGfIe"
      },
      "source": [
        "#### Housing dataset\n",
        "[dataset description](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8R1xUMWGfIe"
      },
      "source": [
        "data = datasets.load_boston()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U3K4SBRGfIe",
        "outputId": "f7bbbe11-0e2a-4249-bc11-815c0f5539e2"
      },
      "source": [
        "X = data['data']\n",
        "y = data['target']\n",
        "print(f\"Feature list: {data['feature_names']}\")\n",
        "print(data['DESCR'])\n",
        "# plt.plot(data['feature_names'][0])\n",
        "# plt.ylabel('CRIM')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature list: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n",
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYvQkJxfGfIg"
      },
      "source": [
        "class MyLinearRegression:\n",
        "    def __init__(self, weights=0, cost = 0, b = 0,\n",
        "                 regularization_weight=0, method='normal', learning_rate=1e-3):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.weights = weights\n",
        "    def fit(self, X_train, y_train):\n",
        "        if (X_train.dtype is torch.float64 and y_train.dtype is torch.float64):\n",
        "          print(\"Everything is fine\")\n",
        "          return\n",
        "        self.X_train = torch.from_numpy(X_train)\n",
        "        self.y_train = torch.from_numpy(y_train)\n",
        "        print(\"Changing type\")\n",
        "        return        \n",
        "    def predict(self, X):\n",
        "        return torch.matmul(self.X_train,self.weights.t())  + self.b\n",
        "\n",
        "    def cost(self, X, y):\n",
        "        weights = self.weights\n",
        "        X = torch.from_numpy(X)\n",
        "        y = torch.from_numpy(y)\n",
        "        self.cost = torch.matmul((torch.matmul(X,weights) - y).t(),(torch.matmul(X,weights) - y))\n",
        "    def normal_equation(self, X, y):\n",
        "        X = self.X_train\n",
        "        y = self.y_train\n",
        "        self.weights = torch.matmul(torch.inverse(torch.matmul(X.t(),X)),torch.matmul(X.t(),y))\n",
        "        self.b = y - torch.matmul(X,self.weights)\n",
        "    def gradient_descent(self,X, y):\n",
        "      m_current = torch.zeros(13,166, dtype=torch.double)\n",
        "      N = y.size\n",
        "      X = self.X_train\n",
        "      y = self.y_train\n",
        "      b_current = 0\n",
        "      for i in range(1000):\n",
        "        y_current = (torch.matmul(m_current, X)) + b_current\n",
        "        y - y_current\n",
        "        cost = torch.sum([data**2 for data in (y-y_current)]) / N\n",
        "        m_gradient = -(2/N) * torch.sum(torch.matmul(X,(y - y_current)))\n",
        "        b_gradient = -(2/N) * torch.sum(y - y_current)\n",
        "        m_current = m_current - (learning_rate * m_gradient)\n",
        "        b_current = b_current - (learning_rate * b_gradient)\n",
        "      self.weights = m\n",
        "      self.b = b_current\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBh5w6VvkVq9"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.67, random_state=1)\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fuELk7UGfIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5c937f-44a4-48d1-d15f-50c04a0ee14f"
      },
      "source": [
        "#@title\n",
        "model = MyLinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "model.normal_equation(X_train, y_train)\n",
        "\n",
        "model.predict(X_train)\n",
        "\n",
        "# model.cost(X_train, y_train)\n",
        "# model.gradient_descent(X_train,y_train)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Changing type\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18.5000, 29.1000, 20.4000, 11.3000, 17.4000,  8.7000, 18.9000, 23.2000,\n",
              "        22.2000, 29.1000, 34.6000, 25.0000, 23.2000, 37.9000,  7.0000, 18.2000,\n",
              "        19.3000, 26.7000, 19.2000, 30.1000, 20.6000, 50.0000, 18.7000, 20.6000,\n",
              "        31.1000, 14.0000, 17.8000, 42.3000, 15.3000, 18.5000, 21.4000, 15.0000,\n",
              "        20.7000, 21.4000, 21.7000, 22.0000, 31.6000, 22.0000, 10.2000, 22.6000,\n",
              "        20.0000, 17.8000, 13.6000, 11.8000, 19.4000, 21.4000, 32.9000, 20.8000,\n",
              "        31.0000, 17.5000, 15.4000, 10.8000, 34.7000, 25.0000, 48.8000, 42.8000,\n",
              "        19.5000, 30.1000, 22.2000, 50.0000, 23.1000, 32.5000, 19.6000, 14.9000,\n",
              "        26.4000, 37.0000, 24.1000, 24.5000, 23.7000,  7.0000, 22.2000, 23.3000,\n",
              "        15.6000, 13.4000, 30.7000, 22.3000, 17.4000, 50.0000, 22.9000, 19.7000,\n",
              "        15.6000, 17.8000, 10.9000, 35.1000, 15.7000, 50.0000, 22.8000, 19.9000,\n",
              "        20.1000, 19.4000, 46.0000, 23.2000, 37.6000, 23.1000, 13.9000, 33.3000,\n",
              "        33.0000, 19.9000, 20.3000, 50.0000, 19.4000, 19.5000, 22.8000, 16.6000,\n",
              "        20.0000, 24.7000, 45.4000, 33.4000, 21.4000, 19.4000,  5.0000,  7.4000,\n",
              "        20.1000, 12.7000, 20.3000, 14.1000, 18.3000, 19.9000, 23.3000, 36.5000,\n",
              "        20.0000, 17.8000,  8.8000, 21.6000, 21.6000, 15.2000, 19.8000, 21.0000,\n",
              "        27.1000, 16.8000, 14.4000, 22.5000, 18.6000, 20.1000, 19.6000, 25.0000,\n",
              "        17.4000, 19.7000,  5.0000, 16.3000, 13.1000, 29.6000, 13.1000, 19.1000,\n",
              "        12.1000, 21.7000, 21.9000, 33.2000, 29.9000, 35.4000, 15.1000, 31.5000,\n",
              "        21.7000, 16.4000, 14.3000, 11.8000, 14.1000, 21.1000, 18.4000, 48.5000,\n",
              "        13.8000, 20.9000, 22.8000, 12.5000, 24.0000, 21.0000],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}