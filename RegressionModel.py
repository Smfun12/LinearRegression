# -*- coding: utf-8 -*-
"""Practice1_LinReg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zI6dKOds8_CPi9llqj0NjluIqERiAbdh

## Practice 1: Linear Regression <br>
<br>
<br>

1. Read data, analyze it, split it into train and test set using scikit-learn train_test_split. Read about why we do this.
[why split the data](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/) <br>
2. Implement your own linear regression. (numpy or torch)<br>
3. Try calculating it using <b>Normal equation</b> VS <b>Gradient descent</b>. Try implementing it using only the formulas. Which one is faster? In which situation is gradient descent faster than normal equation?<br>
4. Compare performance to scikit-learn LinearRegression. <br>
5. Try adding in L1 or L2 regularization. Try different regularization weights. Does it help? Read about regularization and why we do it. [regularization](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) <br>
6. Try scaling your data using scikit learn StandardScaler or other techniques [data scaling](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)

Might find useful [hands on ML](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
"""

import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

"""#### Housing dataset
[dataset description](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)
"""

data = datasets.load_boston()

X = data['data']
y = data['target']
print(f"Feature list: {data['feature_names']}")
print(data['DESCR'])
# plt.plot(data['feature_names'][0])
# plt.ylabel('CRIM')

class MyLinearRegression:
    def __init__(self, weights=0, cost = 0, b = 0,
                 regularization_weight=0, method='normal', learning_rate=1e-3):
        self.X_train = X_train
        self.y_train = y_train
        self.weights = weights
    def fit(self, X_train, y_train):
        if (X_train.dtype is torch.float64 and y_train.dtype is torch.float64):
          print("Everything is fine")
          return
        self.X_train = torch.from_numpy(X_train)
        self.y_train = torch.from_numpy(y_train)
        print("Changing type")
        return        
    def predict(self, X):
        X_train = torch.from_numpy(X)
        self.weights*X_train  + self.b

    def cost(self, X, y):
        weights = self.weights
        X = torch.from_numpy(X)
        y = torch.from_numpy(y)
        self.cost = torch.matmul((torch.matmul(X,weights) - y).t(),(torch.matmul(X,weights) - y))
    def normal_equation(self, X, y):
        X = torch.from_numpy(X)
        y = torch.from_numpy(y)
        self.weights = torch.matmul(torch.inverse(torch.matmul(X.t(),X)),torch.matmul(X.t(),y))
        self.b = y - torch.matmul(X,self.weights)
    def gradient_descent(self,X, y):
      m_current = torch.zeros(13,166, dtype=torch.double)
      N = X.shape[0]
      X = torch.from_numpy(X)
      y = torch.from_numpy(y)
      b_current = 0
      for i in range(1000):
        y_current = (torch.matmul(m_current, X)) + b_current
        cost = torch.sum([data**2 for data in (y-y_current)]) / N
        m_gradient = -(2/N) * torch.sum(torch.matmul(X,(y - y_current)))
        b_gradient = -(2/N) * torch.sum(y - y_current)
        m_current = m_current - (learning_rate * m_gradient)
        b_current = b_current - (learning_rate * b_gradient)
      self.weights = m
      self.b = b_current

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.67, random_state=1)

#@title
model = MyLinearRegression()
model.fit(X_train,y_train)
model.normal_equation(X_train, y_train)

model.predict(X_train)

# model.cost(X_train, y_train)
# model.gradient_descent(X_train,y_train)
